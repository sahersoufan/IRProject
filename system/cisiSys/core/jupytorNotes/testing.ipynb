{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLower(text):\n",
    "    try:\n",
    "        return text.lower()\n",
    "    except: \n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "import re\n",
    "reg = r'([0-9]+)'\n",
    "\n",
    "def isFLoat(strNum):\n",
    "    try:\n",
    "        float(strNum)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def converteNumbers(text):\n",
    "    try:\n",
    "        tempText = text.split()\n",
    "        newText = []\n",
    "        for word in tempText:\n",
    "            tempList = re.split(reg,word)\n",
    "            for miniWord in tempList:\n",
    "                if miniWord.isdigit() or isFLoat(miniWord):\n",
    "                    temp = p.number_to_words(miniWord)\n",
    "                    newText.append(removePunctuation(temp))\n",
    "                else:\n",
    "                    newText.append(miniWord)        \n",
    "        tempText = ' '.join(newText)\n",
    "        return tempText\n",
    "    except:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "translator = str.maketrans(string.punctuation,' '*len(string.punctuation))\n",
    "def removePunctuation(text):\n",
    "    try:\n",
    "        global translator\n",
    "        return text.translate(translator)\n",
    "    except:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeWhiteSpace(text):\n",
    "    return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def removeStopWords(text):\n",
    "    try:\n",
    "        sw = set(stopwords.words(\"english\"))\n",
    "        wt = word_tokenize(text)\n",
    "        filteredText = [word for word in wt if word not in sw]\n",
    "        return ' '.join(filteredText)\n",
    "    except:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stemWords(text):\n",
    "    try:\n",
    "        global stemmer\n",
    "        wt = word_tokenize(text)\n",
    "        stems = []\n",
    "        for word in wt:\n",
    "            temp = stemmer.stem(word)\n",
    "            stems.append(temp)\n",
    "        return ' '.join(stems)\n",
    "    except:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag, defaultdict\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lemmatizeWords(text):\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "        lemmas = [lmtzr.lemmatize(token, tag_map[tag[0]]) for token, tag in pos_tag(tokens) ]\n",
    "        return ' '.join(lemmas)\n",
    "    except:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# doit = [0,0,0,0,0,0,0]\n",
    "def TitlePreProcesse(t, doit):\n",
    "    tempText = t\n",
    "    tempText = toLower(tempText)  if doit[0] == 1 else tempText\n",
    "    tempText = removePunctuation(tempText) if doit[1] == 1 else tempText\n",
    "    tempText = converteNumbers(tempText) if doit[2] == 1 else tempText\n",
    "    tempText = removeWhiteSpace(tempText) if doit[3] == 1 else tempText\n",
    "    tempText = removeStopWords(tempText) if doit[4] == 1 else tempText\n",
    "    tempText = stemWords(tempText) if doit[5] == 1 else tempText\n",
    "    tempText = lemmatizeWords(tempText) if doit[6] == 1 else tempText\n",
    "    return tempText\n",
    "\n",
    "def abstractPreProcesse(a, doit):\n",
    "    tempText = a\n",
    "    tempText = toLower(tempText)  if doit[0] == 1 else tempText\n",
    "    tempText = removePunctuation(tempText) if doit[1] == 1 else tempText\n",
    "    tempText = converteNumbers(tempText) if doit[2] == 1 else tempText\n",
    "    tempText = removeWhiteSpace(tempText) if doit[3] == 1 else tempText\n",
    "    tempText = removeStopWords(tempText) if doit[4] == 1 else tempText\n",
    "    tempText = stemWords(tempText) if doit[5] == 1 else tempText\n",
    "    tempText = lemmatizeWords(tempText) if doit[6] == 1 else tempText\n",
    "    return tempText\n",
    "\n",
    "# i didn't do it yet on cisi (converte date to timestamp)\n",
    "def publicationPreProcesse(p, doit): \n",
    "    try:\n",
    "        return pd.to_datetime(p)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def authorPreProcesse(a, doit):\n",
    "    tempText = a\n",
    "    tempText = toLower(a)  if doit[0] == 1 else tempText\n",
    "    lis = tempText.split(' ')\n",
    "    names = ' '\n",
    "    l = []\n",
    "    for word in lis:\n",
    "      if ',' in word:\n",
    "          l.append(removePunctuation(word.replace(',','')))\n",
    "    names = ' '.join(l)\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def preprocessedData(dataFrame:pd.DataFrame, doit):\n",
    "    pdataFrame = pd.DataFrame()\n",
    "    for i in dataFrame.index:\n",
    "        seriesDict = {}\n",
    "        try:\n",
    "            tempT = tempA = tempB = tempW = None\n",
    "            if not dataFrame.loc[i, '.T'] == '':\n",
    "                tempT = TitlePreProcesse(dataFrame.loc[i, '.T'], doit)\n",
    "            if not dataFrame.loc[i, '.A'] == '':\n",
    "                tempA = authorPreProcesse(dataFrame.loc[i, '.A'], doit)\n",
    "            if not dataFrame.loc[i, '.B'] == '':\n",
    "                tempB = publicationPreProcesse(dataFrame.loc[i, '.B'], doit)\n",
    "            if not dataFrame.loc[i, '.W'] == '':\n",
    "                tempW = abstractPreProcesse(dataFrame.loc[i, '.W'], doit)\n",
    "\n",
    "            seriesDict['.I'] = i\n",
    "            seriesDict['.T'] = tempT\n",
    "            seriesDict['.A'] = tempA\n",
    "            seriesDict['.B'] = tempB\n",
    "            seriesDict['.W'] = tempW\n",
    "            \n",
    "            pdataFrame = pdataFrame.append(seriesDict, ignore_index=True)\n",
    "        except:\n",
    "            print(i)\n",
    "            raise \n",
    "    \n",
    "    pdataFrame.fillna('', inplace=True)\n",
    "    return pdataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def qTitlePreProcesse(t, doit):\n",
    "    tempText = t\n",
    "    tempText = toLower(tempText)  if doit[0] == 1 else tempText\n",
    "    tempText = removePunctuation(tempText) if doit[1] == 1 else tempText\n",
    "    tempText = converteNumbers(tempText) if doit[2] == 1 else tempText\n",
    "    tempText = removeWhiteSpace(tempText) if doit[3] == 1 else tempText\n",
    "    tempText = removeStopWords(tempText) if doit[4] == 1 else tempText\n",
    "    tempText = stemWords(tempText) if doit[5] == 1 else tempText\n",
    "    tempText = lemmatizeWords(tempText) if doit[6] == 1 else tempText\n",
    "    return tempText\n",
    "\n",
    "def qAbstractPreProcesse(a, doit):\n",
    "    tempText = a\n",
    "    tempText = toLower(tempText)  if doit[0] == 1 else tempText\n",
    "    tempText = removePunctuation(tempText) if doit[1] == 1 else tempText\n",
    "    tempText = converteNumbers(tempText) if doit[2] == 1 else tempText\n",
    "    tempText = removeWhiteSpace(tempText) if doit[3] == 1 else tempText\n",
    "    tempText = removeStopWords(tempText) if doit[4] == 1 else tempText\n",
    "    tempText = stemWords(tempText) if doit[5] == 1 else tempText\n",
    "    tempText = lemmatizeWords(tempText) if doit[6] == 1 else tempText\n",
    "    return tempText\n",
    "\n",
    "def qAuthorPreProcesse(a, doit):\n",
    "    tempText = a\n",
    "    tempText = toLower(tempText) if doit[0] == 1 else tempText\n",
    "    lis = tempText.split(' ')\n",
    "    names = ' '\n",
    "    l = []\n",
    "    for word in lis:\n",
    "      if ',' in word:\n",
    "          l.append(removePunctuation(word.replace(',','')))\n",
    "    names = ' '.join(l)\n",
    "    return names\n",
    "\n",
    "\n",
    "regexPub = r'[0-9]{4}'\n",
    "# i didn't do it yet on cisi (converte date to timestamp)\n",
    "def qPublicationPreProcesse(p, doit):\n",
    "    tempText = p.split(',')[3]\n",
    "    try:\n",
    "        return pd.to_datetime(tempText)\n",
    "    except TypeError:\n",
    "        return pd.to_datetime(str(re.search(regexPub, tempText)))\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def preprocesseQuery(dataFrame:pd.DataFrame, doit):\n",
    "    pdataFrame = pd.DataFrame() \n",
    "    for i in dataFrame.index:\n",
    "        seriesDict = {} \n",
    "\n",
    "        try:\n",
    "            templist = []\n",
    "            tempT = tempA = tempW = tempB = None\n",
    "            if not dataFrame.loc[i, '.T'] == '':\n",
    "                tempT = qTitlePreProcesse(dataFrame.loc[i, '.T'], doit)\n",
    "            if not dataFrame.loc[i, '.A'] == '':\n",
    "                tempA = qAuthorPreProcesse(dataFrame.loc[i, '.A'], doit)\n",
    "            if not dataFrame.loc[i, '.W'] == '':\n",
    "                tempW = qAbstractPreProcesse(dataFrame.loc[i, '.W'], doit)\n",
    "            if not dataFrame.loc[i, '.B'] == '':\n",
    "                tempB = qPublicationPreProcesse(dataFrame.loc[i, '.B'], doit)\n",
    "\n",
    "\n",
    "            seriesDict['.I'] = i\n",
    "            seriesDict['.T'] = tempT\n",
    "            seriesDict['.A'] = tempA\n",
    "            seriesDict['.W'] = tempW\n",
    "            seriesDict['.B'] = tempB\n",
    "            pdataFrame = pdataFrame.append(seriesDict, ignore_index=True)\n",
    "        except:\n",
    "            print(i)\n",
    "            raise \n",
    "    pdataFrame.fillna('', inplace=True)\n",
    "    return pdataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "    \n",
    "def makeTransformer():\n",
    "    transformer = FeatureUnion([\n",
    "                    ('title_tfidf', \n",
    "                      Pipeline([('extract_field',\n",
    "                                  FunctionTransformer(lambda x: x['.T'], \n",
    "                                                      validate=False)),\n",
    "                                ('tfidf', \n",
    "                                  TfidfVectorizer())])),\n",
    "                    ('author_tfidf', \n",
    "                      Pipeline([('extract_field', \n",
    "                                  FunctionTransformer(lambda x: x['.A'], \n",
    "                                                      validate=False)),\n",
    "                                ('tfidf', \n",
    "                                  TfidfVectorizer())])),\n",
    "                    ('abstract_tfidf',\n",
    "                     Pipeline([('extract_field',\n",
    "                                FunctionTransformer(lambda x: x['.W'],\n",
    "                                                      validate=False)),\n",
    "                                ('tfidf',\n",
    "                                  TfidfVectorizer())]))])\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "\n",
    "def search(query,table,transformer,n:int):\n",
    "    querytfidf = transformer.transform(query)\n",
    "\n",
    "    cos = cosine_similarity(querytfidf,table).flatten()\n",
    "\n",
    "    return cos.argsort(axis=0)[-n:][::-1]\n",
    "\n",
    "    \n",
    "def queryingData(qDataFrame:pd.DataFrame, aDataFrame:pd.DataFrame, tfidfTable,transformer, n):\n",
    "    result = pd.DataFrame()\n",
    "    resultDict:dict = {}\n",
    "    resultDictCopy = resultDict.copy()\n",
    "    for i in qDataFrame.index:\n",
    "        try:\n",
    "            tempList:list = search(pd.DataFrame(qDataFrame.loc[qDataFrame.index == i,:]),tfidfTable,transformer, n)\n",
    "         \n",
    "            ids = []\n",
    "            for i in tempList:\n",
    "                ids.append(aDataFrame.loc[i,'.I'])\n",
    "\n",
    "            for id in range(1,n+1):\n",
    "                resultDictCopy[str(id)] = ids[id - 1]\n",
    "\n",
    "            result = result.append(resultDictCopy, ignore_index=True)\n",
    "            resultDictCopy = resultDict.copy()\n",
    "        except:\n",
    "            print(i)\n",
    "            raise\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def reSizeLists(l1:list, l2:list):\n",
    "    '''resize lists to have the same len'''\n",
    "    if len(l1) < len(l2):\n",
    "        l2 = l2[0:len(l1)]\n",
    "    while len(l1) > len(l2):\n",
    "        l1 = l1[0:len(l2)]\n",
    "\n",
    "    return l1, l2\n",
    "\n",
    "def calcAPrecisionAtK(resData:pd.DataFrame, rel):\n",
    "    '''calcualte Average Precision'''\n",
    "    precisionsAtK:list = []\n",
    "\n",
    "    for i in resData.index:\n",
    "\n",
    "        resArray = resData.loc[i].to_numpy()\n",
    "        qresArray = np.array(rel)\n",
    "        \n",
    "        if len(qresArray) == 0:\n",
    "            continue\n",
    "\n",
    "        resArray, qresArray = reSizeLists(resArray, qresArray)\n",
    "        \n",
    "        return precision_score(qresArray, resArray, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cisiDataCleaned = pd.read_csv('../../cisiData/cisiDataCleaned.csv')\n",
    "cisiDataCleaned.fillna('', inplace=True)\n",
    "\n",
    "query = pd.read_csv('../../cisiData/testQuery.csv')\n",
    "query.fillna('', inplace=True)\n",
    "\n",
    "# rq = pd.read_csv('../../cisiData/cisiQRels.csv')\n",
    "# rq.fillna('', inplace=True)\n",
    "\n",
    "rel = [28,\n",
    "35,\n",
    "38,\n",
    "42,\n",
    "43,\n",
    "52,\n",
    "65,\n",
    "76,\n",
    "86,\n",
    "150,\n",
    "189,\n",
    "192,\n",
    "193,\n",
    "195,\n",
    "215,\n",
    "269,\n",
    "291,\n",
    "320,\n",
    "429,\n",
    "465,\n",
    "466,\n",
    "482,\n",
    "483,\n",
    "510,\n",
    "524,\n",
    "541,\n",
    "576,\n",
    "582,\n",
    "589,\n",
    "603,\n",
    "650,\n",
    "680,\n",
    "711,\n",
    "722,\n",
    "726,\n",
    "783,\n",
    "813,\n",
    "820,\n",
    "868,\n",
    "869,\n",
    "894,\n",
    "1162,\n",
    "1164,\n",
    "1195,\n",
    "1196,\n",
    "1281]\n",
    "\n",
    "\n",
    "dolist = []\n",
    "precList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "doit = [1,1,1,1,1,1,1]\n",
    "\n",
    "\n",
    "pcd =  preprocessedData(cisiDataCleaned, doit)      \n",
    "pq = preprocesseQuery(query, doit)\n",
    "\n",
    "trans = makeTransformer()\n",
    "tfidfTable = trans.fit_transform(pcd)\n",
    "\n",
    "queryResult = queryingData(pq, pcd, tfidfTable,trans, 50)\n",
    "prec = calcAPrecisionAtK(queryResult, rel)\n",
    "\n",
    "print(doit)\n",
    "print(prec)\n",
    "dolist.append(doit)\n",
    "precList.append(prec)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ad1a5a807b944b1335f606d031e49130ad1da3a9de40b9fa5d942006ec880ff"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('IRProject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
